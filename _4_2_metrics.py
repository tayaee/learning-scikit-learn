"""
모델 선택 및 평가 도구
    모델의 성능을 평가하고 최적의 하이퍼파라미터를 찾는 데 도움이 되는 API를 제공합니다.
        sklearn.model_selection: 데이터를 훈련 세트와 테스트 세트로 나누거나,
            교차 검증(cross_val_score, KFold)을 수행하는 데 사용됩니다.
        sklearn.metrics: 모델의 성능을 평가하는 다양한 지표를 제공합니다.
            정확도(accuracy_score), 정밀도(precision_score), 재현율(recall_score), F1 점수(f1_score) 등

오차 행렬(Confusion Matrix)의 이해:
    이 네 가지 지표는 오차 행렬의 4가지 값 (참 양성, 거짓 양성, 거짓 음성, 참 음성)을 조합하여 계산됩니다.
    따라서 오차 행렬을 먼저 이해하고 나면, 각 지표의 공식과 의미를 쉽게 파악할 수 있습니다.

지표 간의 상충 관계(Trade-off):
    정밀도(Precision)와 재현율(Recall)은 서로 상충 관계에 있습니다.
    즉, 한 쪽이 높아지면 다른 한 쪽이 낮아지는 경향이 있습니다.
    이 관계를 이해하지 못하고 한 가지 지표만 보면 모델의 진짜 성능을 오해할 수 있습니다.
    예를 들어, 무조건 '양성'이라고 예측하는 모델은 재현율은 높지만 정밀도는 매우 낮을 것입니다.
    F1 점수는 바로 이 정밀도와 재현율의 균형을 잡아주는 지표이므로, 세 가지를 함께 이해해야 의미가 명확해집니다.

다양한 평가 기준의 필요성:
    **정확도(Accuracy)**는 가장 직관적이지만, 클래스 불균형(Class Imbalance) 문제에서는
    모델의 성능을 제대로 평가하지 못하는 한계가 있습니다.
    예를 들어, 100개 중 99개가 음성 클래스인 데이터셋에서 모델이 항상 '음성'이라고 예측하면
    정확도는 99%가 나오지만, 실제로는 아무것도 학습하지 않은 무의미한 모델입니다.
    이때 정밀도, 재현율, F1 점수가 중요한 역할을 합니다. 이 지표들을 함께 봐야 모델이 소수 클래스를 얼마나 잘 예측하는지 알 수 있습니다.

학습 권장 순서
    1. **오차 행렬(Confusion Matrix)**의 4가지 구성 요소 (TP, FP, FN, TN)의 개념을 완벽하게 이해합니다.
    2. accuracy_score, precision_score, recall_score, f1_score를 각각 오차 행렬의 값들로 어떻게 계산하는지 공식을 확인합니다.
    3. 각 지표가 어떤 의미를 가지며, 어떤 상황에서 중요한 지표가 되는지 예시를 통해 학습합니다.
        정밀도는 '거짓 양성(FP)'를 줄이는 것이 중요한 상황 (예: 스팸 메일 분류).
        재현율은 '참 양성(TP)'을 놓치지 않는 것이 중요한 상황 (예: 암 진단).
        F1 점수는 정밀도와 재현율이 모두 중요한 상황.
    4. 마지막으로, 실제 코드에서 이 네 가지 지표를 한 번에 계산하고 해석하는 연습을 합니다.

이 데모 코드는 sklearn.metrics 모듈의 주요 분류 평가 지표를 통합하여 보여줍니다.
특히 **오차 행렬(Confusion Matrix)**의 개념을 기반으로 각 지표의 의미와 사용 사례를 명확히 이해할 수 있도록 구성했습니다.
"""

# 1. 필요한 라이브러리 임포트
from sklearn.datasets import make_classification
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import (
    accuracy_score,
    confusion_matrix,
    f1_score,
    precision_score,
    recall_score,
)
from sklearn.model_selection import train_test_split

# 2. 예제 데이터 생성
# 클래스 불균형이 있는 데이터를 생성합니다. (클래스 0: 900개, 클래스 1: 100개)
# 이 데이터셋을 통해 정확도의 한계를 파악할 수 있습니다.
X, y = make_classification(
    n_samples=1000,
    n_features=20,
    n_classes=2,
    n_informative=10,
    n_redundant=0,
    weights=[0.9, 0.1],  # 클래스 불균형 설정
    random_state=42,
)

# 3. 데이터 분할 및 모델 학습
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
model = LogisticRegression(random_state=42)
model.fit(X_train, y_train)
y_pred = model.predict(X_test)

# 4. 오차 행렬 (Confusion Matrix) 생성 및 분석
# 오차 행렬은 모델의 예측 결과가 실제 정답과 얼마나 일치하는지 표 형태로 나타냅니다.
# 오차 행렬의 구성:
# [TN, FP]
# [FN, TP]
# - TP (True Positive): 실제 정답이 Positive이고 예측도 Positive (정확히 맞힘)
# - FN (False Negative): 실제 정답이 Positive인데 예측은 Negative (틀림, 놓침)
# - FP (False Positive): 실제 정답이 Negative인데 예측은 Positive (틀림, 잘못 예측함)
# - TN (True Negative): 실제 정답이 Negative이고 예측도 Negative (정확히 맞힘)
conf_matrix = confusion_matrix(y_test, y_pred)
print("=== 오차 행렬 (Confusion Matrix) ===")
print(conf_matrix)
print(f"TP (참 양성): {conf_matrix[1, 1]}")
print(f"FN (거짓 음성): {conf_matrix[1, 0]}")
print(f"FP (거짓 양성): {conf_matrix[0, 1]}")
print(f"TN (참 음성): {conf_matrix[0, 0]}")
print("-" * 50)


# 5. 다양한 평가 지표 계산 및 해석
# 이 지표들은 오차 행렬의 값들을 조합하여 계산됩니다.

# 5-1. 정확도 (Accuracy)
# 전체 예측 중 올바르게 예측한 비율. (TP + TN) / (TP + FN + FP + TN)
# 클래스 불균형이 있을 때 모델 성능을 왜곡할 수 있습니다.
accuracy = accuracy_score(y_test, y_pred)
print(f"정확도 (Accuracy): {accuracy:.4f}")

# 5-2. 정밀도 (Precision)
# 모델이 '양성'으로 예측한 것 중에서 실제 정답이 '양성'인 비율. TP / (TP + FP)
# 거짓 양성(FP)을 줄이는 것이 중요한 상황에 사용됩니다 (예: 스팸 메일, 암호화폐 사기 탐지).
precision = precision_score(y_test, y_pred)
print(f"정밀도 (Precision): {precision:.4f}")

# 5-3. 재현율 (Recall)
# 실제 정답이 '양성'인 것 중에서 모델이 '양성'으로 올바르게 예측한 비율. TP / (TP + FN)
# 거짓 음성(FN)을 줄이는 것이 중요한 상황에 사용됩니다 (예: 암 진단, 재난 예측).
recall = recall_score(y_test, y_pred)
print(f"재현율 (Recall): {recall:.4f}")

# 5-4. F1 점수 (F1 Score)
# 정밀도와 재현율의 조화 평균. 2 * (정밀도 * 재현율) / (정밀도 + 재현율)
# 정밀도와 재현율이 균형 있게 높을 때 높은 값을 가집니다. 두 지표가 모두 중요한 상황에 사용됩니다.
f1 = f1_score(y_test, y_pred)
print(f"F1 점수 (F1 Score): {f1:.4f}")
print("-" * 50)

"""
실행 예시

=== 오차 행렬 (Confusion Matrix) ===
[[262   6]
 [ 22  10]]
TP (참 양성): 10
FN (거짓 음성): 22
FP (거짓 양성): 6
TN (참 음성): 262
--------------------------------------------------
정확도 (Accuracy): 0.9067
정밀도 (Precision): 0.6250
재현율 (Recall): 0.3125
F1 점수 (F1 Score): 0.4167
--------------------------------------------------
"""

"""
복습

Q1. 데모 코드에서 accuracy_score가 높게 나왔음에도 불구하고, 모델이 완벽하다고 말할 수 없는 이유는 무엇인가요?

    데모 코드는 클래스 불균형 데이터셋을 사용했습니다. 전체 데이터의 90%가 클래스 0, 10%가 클래스 1입니다.

    만약 모델이 모든 예측을 무조건 '클래스 0'으로만 한다면, 정확도는 90%에 가깝게 나옵니다.
    
    이처럼 정확도는 클래스가 불균형할 경우 모델의 실제 성능을 제대로 반영하지 못하는 한계가 있습니다. 
    소수 클래스(클래스 1)에 대한 예측 능력이 전혀 없더라도 높은 정확도를 보일 수 있기 때문입니다.

Q2. 정밀도와 재현율의 정의를 오차 행렬의 구성 요소(TP, FP, FN, TN)를 사용해 설명하고, 각각이 중요한 실제 사례를 들어주세요.
    
    정밀도 (Precision): TP / (TP + FP)
        모델이 "양성"이라고 예측한 결과들 중에서 실제로 정답이 "양성"인 비율입니다. 거짓 양성(FP)을 최소화하는 것이 중요할 때 사용됩니다.
        사례: 스팸 메일 필터링. 스팸이 아닌 중요한 메일(음성)을 스팸(양성)으로 잘못 분류(FP)하면 안 되기 때문에, 정밀도가 높아야 합니다.
    재현율 (Recall): TP / (TP + FN)
        실제 정답이 "양성"인 것들 중에서 모델이 "양성"이라고 올바르게 예측한 비율입니다. 거짓 음성(FN)을 최소화하는 것이 중요할 때 사용됩니다.
        사례: 암 진단. 실제 암 환자(양성)를 암이 아니라고 진단(FN)하면 치명적이기 때문에, 모든 암 환자를 놓치지 않고 찾아내는(높은 재현율) 것이 중요합니다.

Q3. F1 점수는 왜 정밀도와 재현율의 '조화 평균'으로 계산되나요? 그냥 산술 평균을 사용하지 않는 이유는 무엇인가요?

    산술 평균은 극단적인 값에 의해 왜곡될 수 있습니다. 
    예를 들어, 정밀도 100%, 재현율 0%인 경우 산술 평균은 50%가 되지만, 
    이 모델은 실제로는 아무것도 예측하지 못한 무의미한 모델입니다.
    
    조화 평균은 이처럼 한쪽 지표가 0에 가까울 때 전체 점수를 크게 낮춥니다. 
    F1 점수는 정밀도와 재현율 중 낮은 쪽에 더 큰 가중치를 두어, 
    두 지표가 모두 균형 있게 높아야만 높은 점수를 얻을 수 있도록 설계되었습니다.

    조화평균을 계산하는 식은 다음과 같다.

        F1 = 2 * (정밀도 * 재현율) / (정밀도 + 재현율)


Q4. 클래스 불균형 데이터에서 accuracy_score 대신 다른 지표들을 함께 보는 것이 왜 중요한가요?
    
    클래스 불균형이 있는 데이터에서는 모델이 다수 클래스만 예측해도 accuracy_score가 높게 나올 수 있습니다. 
    이는 모델이 소수 클래스를 전혀 학습하지 못했음에도 불구하고 좋은 성능을 내는 것처럼 오해하게 만듭니다.

    **정밀도, 재현율, F1 점수**는 소수 클래스에 대한 모델의 예측 능력을 더 자세히 보여줍니다. 
    특히 소수 클래스에 대한 **재현율**이 낮다면, 모델이 해당 클래스의 샘플들을 제대로 찾아내지 못하고 있다는 것을 명확하게 알 수 있습니다. 
    따라서 이 지표들을 함께 보아야 모델의 진정한 성능을 정확히 평가할 수 있습니다.

    한번의 양성진단도 치명적 => 스팸필터 => 정밀도 높은 모델 선택
    양성진단 한번이라도 놓치면 치명적 => 암진단 => 재현율 높은 모델 선택

"""
