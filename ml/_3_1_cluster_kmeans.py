"""
Unsupervised Learning:
    Clustering: Used to find the structure or pattern in data and group similar data points together.
        Examples: sklearn.cluster.KMeans, sklearn.cluster.DBSCAN
    Dimensionality Reduction: Reduces the number of features in data to facilitate analysis and visualization.
        Examples: sklearn.decomposition.PCA, sklearn.manifold.TSNE

KMeans:
    KMeans is one of the most widely used Unsupervised Learning algorithms,
    used to group data into a pre-defined number (K) of Clusters.
    This demo code will walk through the learning process of KMeans step-by-step.
"""

# 1. Import necessary libraries
import matplotlib.pyplot as plt  # Visualization library
from sklearn.cluster import KMeans  # K-Means clustering model
from sklearn.datasets import make_blobs  # Generate virtual data for clustering

# 2. Generate example data
# Simulate a business scenario: 'Customer Segmentation'.
# The make_blobs function creates virtual data points separated into several clusters (blobs).
# - n_samples=300: Data for 300 customers
# - n_features=2: 2 customer behavior metrics (e.g., 'Avg. Monthly Visits', 'Avg. Monthly Purchase Amount')
# - centers=3: 3 customer groups (e.g., 'Loyal Customers', 'Regular Customers', 'Potential Customers')
# - cluster_std: Standard deviation of data within clusters. A smaller value means clusters are more clearly separated.
# - random_state: Seed value for reproducible results
# - y: The true cluster labels generated by make_blobs. Since KMeans is unsupervised, 'y' is not used for training,
#      but is used as a 'reference answer key' later when evaluating or visualizing how closely the model found the correct clusters.
X, y = make_blobs(n_samples=300, n_features=2, centers=3, cluster_std=0.60, random_state=42)

# ==============================================================================
# Section 2-1: Original Data Visualization (For Reference)
# Visualize the data based on the true clusters (y) generated by make_blobs.
# This represents the 'correct answer' cluster structure that KMeans should find.
# ==============================================================================
plt.figure(figsize=(8, 6))
plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap="viridis")
plt.title("Original Data with True Labels (Ground Truth)")
plt.xlabel("Feature 1")
plt.ylabel("Feature 2")
plt.show()

# 3. Create Estimator Instance
# Create the KMeans model.
# n_clusters: The number of clusters to create (this is the K value).
# random_state: Seed value for reproducible results
kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)
# n_init is the number of times the KMeans algorithm will be run with different centroid seeds to find the best result.

# 4. Model Training (fit)
# Use the model.fit(X) method to train the model.
# Since it's unsupervised learning, only the feature data (X) is used, without the correct labels (y).
# Through this process, KMeans analyzes the data patterns to find the optimal cluster centers (centroids).
print("Starting K-Means clustering...")
kmeans.fit(X)
print("K-Means clustering complete!")

# 5. Prediction (predict) or Cluster Label Assignment
# The predict(X) method is used to predict which cluster each data point belongs to.
# After running fit(), the cluster assignment result for each data point is already stored in the `labels_` attribute.
# Therefore, using `labels_` is more efficient than calling `predict()` again to check the cluster results for the training data.
# y_kmeans = kmeans.predict(X)  # This code also returns the same result.
y_kmeans = kmeans.labels_

# 6. Visualize Results
# Visualize the clustering results using matplotlib.
plt.figure(figsize=(8, 6))
plt.scatter(X[:, 0], X[:, 1], c=y_kmeans, s=50, cmap="viridis")

# Plot the center of each cluster.
centers = kmeans.cluster_centers_
plt.scatter(
    centers[:, 0],
    centers[:, 1],
    c="red",
    s=200,
    alpha=0.7,
    marker="X",
    label="Centroids",
)
plt.title("K-Means Clustering Result")
plt.xlabel("Feature 1")
plt.ylabel("Feature 2")
plt.legend()
plt.show()

# 7. Output Cluster Centers and Assigned Labels
print("\nCentroids of each cluster:")
print(centers)
print("\nCluster labels assigned to the first 10 data points:")
print(y_kmeans[:10])

"""
Starting K-Means clustering...
K-Means clustering complete!
"""

"""
Review

Q1. KMeans is unsupervised learning, so why do we receive true labels (y) when generating the example data?

    KMeans is indeed unsupervised because it does not use the true labels (y) at all during the training process (`fit(X)`).
    The reason we receive 'y' is to **evaluate and visualize** how accurately our model found the clusters relative to the true structure after training is complete.
    In other words, 'y' serves as a **'reference answer key'** for measuring model performance.
    In real-world business data, these true labels are usually unavailable.

Q2. What is the role of the n_clusters parameter, and what problem can arise if this value is set incorrectly?

    **n_clusters** is the parameter that specifies the **number of clusters (K)** the KMeans algorithm should divide the data into.
    Setting this value incorrectly can lead to results that do not reflect the data's actual structure.
    For example, if the real data is composed of 4 clusters but n_clusters is set to 2,
    data points that should belong to different clusters may be incorrectly merged into a single cluster.

Q3. After running kmeans.fit(X), why is the kmeans.labels_ attribute used instead of calling kmeans.predict(X) again?

    During the process of calling the `fit()` method, the KMeans model has already calculated which cluster each data point belongs to and stores that result in the `labels_` attribute.
    Therefore, calling the `predict()` method again is redundant, as it essentially retrieves the already computed result.
    It's often more efficient and improves code readability to directly use the `labels_` attribute, which is accessible immediately after training.

Q4. In this example, why do we use the entire dataset (X) for training and prediction instead of splitting into train/test data?

    This is a very important point, and it is because KMeans is an **Unsupervised Learning** algorithm.

    - **Supervised Learning (Classification/Regression)**: It is crucial to evaluate the model's **generalization performance**.
      Data must be split to check how well the model predicts new, unseen data (Test set).

    - **Unsupervised Learning (Clustering)**: The primary goal is to **discover the inherent structure (clusters)** within the *entire* given dataset.
      The concept of evaluating generalization performance is different since there is no 'correct answer'.
      Therefore, the common approach is to use all available data to understand what clusters exist within it.

    While other techniques exist for evaluating clustering results (e.g., Silhouette Score), using the entire dataset is the standard for basic clustering tasks.

Q5. Briefly explain the working principle of the K-Means algorithm.

    K-Means works by repeating the following process:

    1. Initialization: **Randomly select K cluster centers (Centroids)**, where K is the number of clusters specified by the user.
    2. Assignment: Assign all data points to the closest cluster centroid.
    3. Update: Calculate the mean position of the data points assigned to each cluster and update the cluster centers to these new positions.

    This process is repeated until the positions of the cluster centroids no longer change significantly.

Q6. There is a risk of initial centroids being chosen too close together, leading to a local minimum. How is this prevented?

    The KMeans algorithm is sensitive to the initial choice of centroids, and poor initialization can lead to converging on a local minimum.

    **KMeans++ Initialization**: Using the KMeans++ algorithm selects initial centroids more smartly, which reduces the chance of falling into a local minimum.

    This method is applied by default in scikit-learn's KMeans implementation.
"""
