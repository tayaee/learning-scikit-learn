"""
모델 선택 및 평가 도구
    모델의 성능을 평가하고 최적의 하이퍼파라미터를 찾는 데 도움이 되는 API를 제공합니다.
    sklearn.model_selection: 데이터를 훈련 세트와 테스트 세트로 나누거나, 교차 검증(cross_val_score, KFold)을 수행하는 데 사용됩니다.
    sklearn.metrics: 모델의 성능을 평가하는 다양한 지표를 제공합니다. 정확도(accuracy_score), 정밀도(precision_score), 재현율(recall_score), F1 점수(f1_score) 등

train_test_split
    가장 기본이 되는 개념입니다. 모델 검증의 가장 단순한 형태인 학습-평가 과정을 이해하는 데 필수적입니다.
    데이터를 훈련 세트(Train set)와 테스트 세트(Test set)로 나누는 이유, 그리고 테스트 세트를 사용하여 모델의 일반화 성능을 평가하는 개념을 먼저 익혀야 합니다.
    학습 목표: "왜 데이터를 나눠야 하는가?", "과적합(Overfitting)을 어떻게 평가하는가?"에 대한 답을 얻는 것.

KFold
    train_test_split의 한계를 극복하기 위한 방법입니다. 단 한 번의 분할로는 데이터의 특정 부분에 모델이 편향될 수 있습니다.
    KFold는 데이터를 여러 개의 폴드(Fold)로 나누고, 각 폴드를 돌아가며 테스트 세트로 사용하여 모델을 여러 번 학습하고 평가하는 **교차 검증(Cross-Validation)**의 기본 원리를 이해하는 데 중요합니다.
    학습 목표: "단일 분할의 문제점은 무엇인가?", "교차 검증이 왜 더 신뢰할 수 있는가?"에 대한 답을 얻는 것.

cross_val_score
    KFold의 원리를 자동화한 편리한 함수입니다. KFold 객체를 직접 생성하고 반복문을 돌리는 복잡한 과정을 cross_val_score 함수 하나로 간편하게 처리할 수 있습니다.
    이 함수를 학습함으로써, 교차 검증을 실제 코드에서 효율적으로 적용하는 방법을 배우게 됩니다.
    학습 목표: "교차 검증을 어떻게 간편하게 실행하는가?", "여러 번의 평가 점수를 어떻게 해석하는가?"에 대한 답을 얻는 것.

이 데모 코드는 train_test_split, KFold, cross_val_score를 사용하여 모델 검증 과정을 단계적으로 보여줍니다.
각 코드 블록은 모델 성능을 평가하는 방법의 차이점을 명확하게 보여주며, 주석을 통해 교육 효과를 극대화했습니다.
"""

# 1. 필요한 라이브러리 임포트
import numpy as np
from sklearn.datasets import make_classification
from sklearn.linear_model import LogisticRegression  # 분류 모델
from sklearn.metrics import accuracy_score
from sklearn.model_selection import KFold, cross_val_score, train_test_split

# 2. 예제 데이터 생성
# 1000개의 샘플과 2개의 클래스를 가진 분류용 가상 데이터를 만듭니다.
X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)

# 3. 모델 인스턴스 생성
# 로지스틱 회귀 모델을 사용합니다.
model = LogisticRegression(random_state=42)

# ==============================================================================
# 섹션 1: train_test_split을 이용한 단일 모델 평가
# 모델 검증의 가장 기본적인 방법. 데이터를 한 번만 분할하여 학습과 평가를 진행합니다.
# ==============================================================================

print("=== [Section 1] train_test_split을 이용한 단일 모델 평가 ===")

# 데이터를 학습(train) 세트와 테스트(test) 세트로 한 번만 나눕니다.
# test_size=0.3: 전체 데이터의 30%를 테스트 세트로 사용합니다.
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# 학습 세트로 모델을 학습시킵니다.
model.fit(X_train, y_train)

# 테스트 세트로 예측을 수행합니다.
y_pred = model.predict(X_test)

# 예측 결과의 정확도를 평가합니다.
single_accuracy = accuracy_score(y_test, y_pred)
print(f"단일 분할 정확도: {single_accuracy:.4f}")

# 단점: 단일 테스트 세트의 데이터 분포에 따라 결과가 달라질 수 있어 신뢰도가 낮을 수 있습니다.
print("-" * 50)


# ==============================================================================
# 섹션 2: KFold를 이용한 교차 검증 (수동 구현)
# 데이터의 편향에 따른 성능 변동을 줄이기 위해 K개의 폴드로 나누어 여러 번 평가합니다.
# ==============================================================================

print("=== [Section 2] KFold를 이용한 교차 검증 (수동 구현) ===")

# KFold 객체를 생성합니다.
# n_splits=5: 데이터를 5개의 폴드로 나눕니다.
# shuffle=True: 데이터를 섞어서 분할합니다.
kfold = KFold(n_splits=5, shuffle=True, random_state=42)

accuracies_kfold = []

# KFold 객체를 사용하여 반복문을 돌며 교차 검증을 수행합니다.
# kfold.split(X)는 학습/테스트 인덱스를 생성합니다.
# KFold는 이 5개의 폴드를 가지고 5번의 학습-평가 과정을 반복합니다. 각 반복(iteration)은 아래와 같은 방식으로 이루어집니다.
#   1번째 반복:
#       학습용 데이터: 폴드 1, 2, 3, 4 (총 800개)
#       평가용 데이터: 폴드 5 (총 200개)
#       결과: 1번 정확도 점수 기록
#   2번째 반복:
#       학습용 데이터: 폴드 1, 2, 3, 5 (총 800개)
#       평가용 데이터: 폴드 4 (총 200개)
#       결과: 2번 정확도 점수 기록
#   3번째 반복:
#       학습용 데이터: 폴드 1, 2, 4, 5 (총 800개)
#       평가용 데이터: 폴드 3 (총 200개)
#       결과: 3번 정확도 점수 기록
#   4번째 반복:
#       학습용 데이터: 폴드 1, 3, 4, 5 (총 800개)
#       평가용 데이터: 폴드 2 (총 200개)
#       결과: 4번 정확도 점수 기록
#   5번째 반복:
#       학습용 데이터: 폴드 2, 3, 4, 5 (총 800개)
#       평가용 데이터: 폴드 1 (총 200개)
#       결과: 5번 정확도 점수 기록
# 이렇게 5번의 반복이 끝나면, 우리는 총 5개의 정확도 점수를 얻게 됩니다.
# KFold의 최종 결과는 이 5개 점수의 평균을 내어 '교차 검증 평균 정확도'를 계산하는 것입니다.
for train_index, test_index in kfold.split(X):
    # 각 폴드에 해당하는 학습/테스트 데이터 추출
    X_train_k, X_test_k = X[train_index], X[test_index]
    y_train_k, y_test_k = y[train_index], y[test_index]

    # 모델을 매번 새로 초기화하고 학습시킵니다.
    model_k = LogisticRegression(random_state=42)
    model_k.fit(X_train_k, y_train_k)

    # 평가 및 정확도 저장
    y_pred_k = model_k.predict(X_test_k)
    accuracies_kfold.append(accuracy_score(y_test_k, y_pred_k))

print(f"5-폴드 교차 검증 점수: {accuracies_kfold}")
print(f"5-폴드 교차 검증 평균 정확도: {np.mean(accuracies_kfold):.4f}")
print("-" * 50)


# ==============================================================================
# 섹션 3: cross_val_score를 이용한 교차 검증 (자동화)
# 섹션 2의 복잡한 반복문을 한 줄로 간편하게 실행합니다.
# ==============================================================================

print("=== [Section 3] cross_val_score를 이용한 교차 검증 (자동화) ===")

# cross_val_score 함수를 사용합니다.
# 인자: (모델, 전체 특징 데이터, 전체 타겟 데이터, 교차 검증 폴드 수, 평가 지표)
scores = cross_val_score(model, X, y, cv=5, scoring="accuracy")

print(f"cross_val_score로 얻은 5-폴드 교차 검증 점수: {scores}")
print(f"cross_val_score 평균 정확도: {np.mean(scores):.4f}")
print("-" * 50)

"""
실행 예

=== [Section 1] train_test_split을 이용한 단일 모델 평가 ===
단일 분할 정확도: 0.8500
--------------------------------------------------
=== [Section 2] KFold를 이용한 교차 검증 (수동 구현) ===
5-폴드 교차 검증 점수: [0.855, 0.855, 0.875, 0.865, 0.865]
5-폴드 교차 검증 평균 정확도: 0.8630
--------------------------------------------------
=== [Section 3] cross_val_score를 이용한 교차 검증 (자동화) ===
cross_val_score로 얻은 5-폴드 교차 검증 점수: [0.9   0.885 0.875 0.83  0.845]
cross_val_score 평균 정확도: 0.8670
--------------------------------------------------
"""

"""
복습

Q1. train_test_split만으로 모델을 평가하는 것의 주요 한계점은 무엇인가요?

    train_test_split은 데이터를 단 한 번만 학습 세트와 테스트 세트로 나눕니다. 
    이 때문에 분할된 데이터의 분포가 전체 데이터와 다를 경우, 모델 성능이 과대평가되거나 과소평가될 수 있습니다. 
    특히 데이터셋의 크기가 작을 때 이러한 문제가 두드러집니다. 
    단일 평가 결과만으로는 모델의 일반화 성능을 신뢰하기 어렵다는 한계가 있습니다.

Q2. KFold 객체의 n_splits 매개변수는 무엇을 의미하며, 이 값을 변경하면 교차 검증 과정에 어떤 영향을 미치나요?

    **n_splits**는 교차 검증을 위해 전체 데이터를 나누는 폴드(fold)의 개수를 의미합니다. 
    예를 들어 n_splits=5로 설정하면, 데이터는 5개의 폴드로 나뉩니다.

    이 값을 변경하면 **모델 성능 평가치 자체의 편향-분산 트레이드오프**에 영향을 줍니다.
        n_splits가 크면 (예: 10-fold):
            - **평가의 편향(Bias) 감소**: 각 모델이 더 많은 데이터로 학습하므로, 평가 점수가 '전체 데이터로 학습했을 때의 실제 성능'에 더 가까워집니다. (긍정적)
            - **평가의 분산(Variance) 증가**: 하지만 각 폴드의 학습 데이터가 거의 비슷해져(많이 겹쳐서), 모델들이 서로 비슷하게 학습됩니다. 
              이로 인해 데이터 분할 방식에 따라 평가 점수가 크게 튀거나 떨어질 수 있습니다. 즉, **평가 결과의 안정성이 떨어집니다.**
        n_splits가 작으면 (예: 3-fold):
            - **평가의 편향(Bias) 증가**: 각 모델이 더 적은 데이터로 학습하므로, 평가 점수가 '실제 성능'보다 비관적으로(낮게) 나올 수 있습니다.
            - **평가의 분산(Variance) 감소**: 각 폴드의 학습 데이터가 서로 더 다르므로, 여러 번 평가해도 점수가 크게 변하지 않습니다. 즉, **평가 결과의 안정성이 높아집니다.**

    결론: k=5 또는 k=10은 평가의 편향과 분산 사이의 적절한 균형점으로 널리 사용됩니다.

Q3. cross_val_score는 내부적으로 어떤 과정을 거쳐 교차 검증을 수행하나요? KFold를 수동으로 구현하는 것과 비교했을 때 어떤 장점이 있나요?
    
    cross_val_score는 내부적으로 KFold와 같은 교차 검증 전략을 자동으로 사용합니다. 
    cv 인자에 지정된 폴드 수만큼 데이터를 분할하고, 반복적으로 학습과 평가를 수행한 뒤 그 결과를 리스트로 반환합니다.
    
    장점: KFold를 수동으로 구현할 때 필요한 반복문, 데이터 인덱싱, 모델 초기화, 
    점수 저장 등의 번거로운 과정을 한 번의 함수 호출로 처리할 수 있어 코드를 훨씬 간결하고 효율적으로 작성할 수 있습니다.

Q4. 섹션 2와 섹션 3에서 얻은 평균 정확도 점수가 비슷한 이유를 설명해 주세요.
    
    두 섹션 모두 동일한 교차 검증 원리와 동일한 하이퍼파라미터를 가진 모델을 사용했기 때문입니다.
    
    KFold를 수동으로 구현한 섹션 2와 cross_val_score를 사용한 섹션 3은 모두 n_splits=5, random_state=42로 동일한 데이터 분할 방식을 사용했습니다. 따라서 내부적으로 수행되는 학습과 평가 과정이 거의 동일하여 최종적으로 얻는 평균 정확도 점수도 유사한 결과를 보입니다. 이는 cross_val_score가 KFold의 기능을 효과적으로 캡슐화하고 있음을 보여줍니다.

Q5. 교차 검증에서 얻은 점수들의 분산(또는 표준편차)을 어떻게 실용적으로 활용할 수 있나요?

    교차 검증 점수들의 분산(또는 표준편차)은 평가 결과의 **안정성**을 나타내는 중요한 지표이며, 다음과 같이 실용적으로 활용할 수 있습니다.

    1.  **모델 간 성능 비교**:
        - 모델 A: 평균 정확도 0.92, 표준편차 0.01
        - 모델 B: 평균 정확도 0.93, 표준편차 0.05
        모델 B의 평균 점수가 약간 더 높지만, 표준편차가 5배나 큽니다. 
        이는 모델 B의 성능이 데이터 분할에 따라 크게 달라지는 불안정한 모델임을 의미합니다. 
        이런 경우, 약간의 평균 성능을 희생하더라도 더 안정적이고 신뢰할 수 있는 **모델 A를 선택**하는 것이 현명한 결정일 수 있습니다.

    2.  **신뢰 구간(Confidence Interval) 추정**:
        평균 점수와 표준편차를 함께 사용하여 모델의 성능에 대한 신뢰 구간을 대략적으로 추정할 수 있습니다. 
        일반적인 방법은 `평균 점수 ± 2 * 표준편차`를 계산하는 것입니다.
        예를 들어, 평균 정확도가 0.86이고 표준편차가 0.01이라면, 
        모델의 실제 성능은 대략 84%에서 88% 사이에 있을 것이라고 추정할 수 있습니다. 
        이는 단일 점수보다 훨씬 더 많은 정보를 제공합니다.

    3.  **하이퍼파라미터 튜닝**:
        GridSearchCV와 같은 도구를 사용할 때, `mean_test_score`가 가장 높은 조합을 
        무조건 선택하는 것보다 `std_test_score`가 낮은 조합을 함께 고려하는 것이 좋습니다. 
        평균 점수가 비슷하다면, 표준편차가 더 낮은, 즉 더 안정적인 하이퍼파라미터 조합을 선택하는 것이
        과적합을 피하고 일반화 성능을 높이는 데 도움이 됩니다.

    결론적으로, 분산/표준편차는 단독으로 해석하기보다는 **다른 모델이나 하이퍼파라미터 조합과 비교**하거나, 
    **평균 점수와 함께 신뢰도를 판단**하는 보조 지표로 사용할 때 그 가치가 극대화됩니다.

Q6. cross_val_score의 결과로부터 평균점수와 표준편차를 이용하여 모델 정확도에 대해 신뢰구간 추정을 해보자.

    가장 일반적인 방법은

    성능구간 = 평균점수 ± 2 * 표준편차

    0.9  0.885 0.875 0.83 0.845의 평균 = 0.867, 표준편차 = 0.02885

    성능구간은 0.867 ± 0.054

    즉 이 모델의 정확도가 [0.813, 0.921] 사이에 있을 확률은 95%입니다 라고 말할 수 있다.    
"""
